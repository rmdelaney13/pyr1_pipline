# Cluster Setup & Testing Guide

This guide shows you how to set up the pipeline on the cluster and test with kyna_test.

## ğŸ“ Directory Structure on Cluster

```
/projects/ryde3462/
â”œâ”€â”€ software/
â”‚   â””â”€â”€ pyr1_pipeline/              # â† Your GitHub clone (pipeline code)
â”‚       â”œâ”€â”€ docking/
â”‚       â”œâ”€â”€ design/
â”‚       â””â”€â”€ templates/
â”‚
â”œâ”€â”€ kyna_test/                      # â† Your test project
â”‚   â”œâ”€â”€ config.txt                  # â† Copy from pipeline/kyna_test_config.txt
â”‚   â”œâ”€â”€ conformers/
â”‚   â”‚   â””â”€â”€ kyna_conf.sdf          # â† Already there
â”‚   â””â”€â”€ (outputs will be created)
â”‚
â””â”€â”€ (other projects...)

/scratch/alpine/ryde3462/
â””â”€â”€ kyna_test/                      # â† Large outputs go here
    â”œâ”€â”€ docked/
    â””â”€â”€ design/
```

---

## ğŸš€ Step-by-Step Setup

### 1. Copy Config to Your Project Directory

```bash
# On the cluster
cd /projects/ryde3462/kyna_test

# Copy the config from the pipeline repo
cp /projects/ryde3462/software/pyr1_pipeline/kyna_test_config.txt config.txt

# Verify it's there
ls -lh config.txt
```

### 2. Verify Your SDF File

```bash
cd /projects/ryde3462/kyna_test/conformers
ls -lh kyna_conf.sdf

# Should see your kyna_conf.sdf file
```

### 3. Test Step 1: Create Table (Table Generation)

This generates params and PDB files from your SDF.

```bash
cd /projects/ryde3462/kyna_test

# Run create_table.py
python /projects/ryde3462/software/pyr1_pipeline/docking/scripts/create_table.py config.txt
```

**Expected output:**
- `kyna_ligands.csv` - Alignment table
- `kyna_ligands.pkl` - Pickle file
- `conformers/0/` - Directory with:
  - `0.pdb` - Ligand structure
  - `0.params` - Rosetta params
  - `0.sdf` - Individual conformer

**Verify:**
```bash
# Check table was created
cat kyna_ligands.csv

# Check conformer files
ls -lh conformers/0/
```

### 4. Test Step 2: Run Single Docking Task (Local Test)

Test docking locally before submitting SLURM jobs.

```bash
cd /projects/ryde3462/kyna_test

# Run array index 0 locally (no SLURM)
python /projects/ryde3462/software/pyr1_pipeline/docking/scripts/grade_conformers_glycine_shaved.py config.txt 0
```

**Expected output:**
- `/scratch/alpine/ryde3462/kyna_test/docked/array_0/` - Docked PDBs
- Log messages showing docking progress

**This will take a few minutes.** If it works, you're ready for SLURM!

### 5. Test Step 3: Submit Full Docking via SLURM

Now submit the real workflow with SLURM arrays.

```bash
cd /projects/ryde3462/kyna_test

# Submit complete docking workflow (3 array tasks as configured)
python /projects/ryde3462/software/pyr1_pipeline/docking/scripts/run_docking_workflow.py config.txt --slurm
```

**What happens:**
1. Creates alignment table (if not already done)
2. Submits array job (tasks 0-2)
3. Automatically submits clustering job (runs after arrays complete)
4. You get job IDs

**Monitor:**
```bash
# Check job status
squeue -u ryde3462

# Watch array job output
tail -f /scratch/alpine/ryde3462/kyna_test/logs/docking_*.out

# Watch clustering output (after arrays finish)
tail -f /scratch/alpine/ryde3462/kyna_test/logs/clustering_*.out
```

**Results will be in:**
```bash
/scratch/alpine/ryde3462/kyna_test/docked/clustered_final/
```

### 6. Run Everything: SDF â†’ Docking â†’ Design â†’ AF3 (Recommended)

The pipeline runs in 3 phases. Phase 1 uses a SLURM orchestrator job so it
survives SSH disconnects. Phases 2 and 3 are quick commands you run from a
login node after each phase completes.

#### Phase 1: Docking â†’ AF3 Prep (~4 hours, submit-and-forget)

This submits a lightweight orchestrator SLURM job (1 CPU, 4GB, 8hr wall time)
that automatically chains: docking arrays â†’ clustering â†’ MPNN â†’ Rosetta â†’
filter â†’ FASTA â†’ AF3 JSON prep. It stops before submitting AF3 GPU jobs so it
doesn't hold a CPU node while waiting on GPU queue.

```bash
cd /projects/ryde3462/kyna_test
PIPE=/projects/ryde3462/software/pyr1_pipeline

# Full pipeline through AF3 prep (stops before GPU submission)
bash $PIPE/docking/scripts/submit_full_pipeline.sh config.txt \
  --design-args "--skip-af3-submit --skip-af3-analyze"
```

You can log out after this. The orchestrator runs inside SLURM.

**Monitor progress:**
```bash
# Watch the orchestrator output (live)
tail -f /scratch/alpine/ryde3462/kyna_test/logs/pipeline_orchestrator_<JOBID>.out

# Quick status check (stage-by-stage timestamps)
cat /scratch/alpine/ryde3462/kyna_test/logs/pipeline_status.log

# Check all your jobs
squeue -u ryde3462
```

#### Phase 2: Submit AF3 GPU Jobs (seconds, from login node)

Once Phase 1 finishes (check `pipeline_status.log` or `squeue`), submit the
AF3 GPU jobs. This takes seconds â€” no orchestrator needed:

```bash
cd /projects/ryde3462/kyna_test
python $PIPE/design/scripts/run_design_pipeline.py config.txt --af3-submit-only
```

GPU jobs go into the SLURM queue on their own. Wait time depends on GPU
availability â€” check `squeue -u ryde3462` periodically.

#### Phase 3: Analyze AF3 Results (after GPU jobs finish)

Once all AF3 GPU jobs have completed:

```bash
cd /projects/ryde3462/kyna_test
python $PIPE/design/scripts/run_design_pipeline.py config.txt --af3-analyze-only
```

This extracts pLDDT, ipTM, ligand RMSD, and filters by quality cutoffs.
Final results land in `/scratch/.../design/af3_analysis/filtered_metrics.csv`.

#### Other Orchestrator Options

```bash
# Design only (docking already done), through AF3 prep
bash $PIPE/docking/scripts/submit_full_pipeline.sh config.txt \
  --design-only --design-args "--skip-af3-submit --skip-af3-analyze"

# Docking only (skip design entirely)
bash $PIPE/docking/scripts/submit_full_pipeline.sh config.txt --docking-only

# Jump from existing Rosetta output to AF3 prep
bash $PIPE/docking/scripts/submit_full_pipeline.sh config.txt \
  --design-only --design-args "--rosetta-to-af3 --skip-af3-submit --skip-af3-analyze"
```

The `--wait` flag tells the pipeline to poll SLURM and wait for each batch of jobs
(MPNN, Rosetta, AF3) to finish before continuing to the next stage.

**What happens automatically (10 stages):**

| Stage | What | Output |
|-------|------|--------|
| 1 | LigandMPNN design on docked PDBs | `mpnn_output/*.fa` |
| 2 | Rosetta relax of MPNN sequences | `rosetta_output/*.pdb` + `.sc` |
| 3 | Aggregate Rosetta scores | `scores/iteration_1_scores.csv` |
| 4 | Filter by Rosetta metrics | `filtered/filtered.csv` + PDBs |
| 5 | Generate FASTA from filtered PDBs | `filtered/filtered.fasta` |
| 6 | Generate AF3 JSON inputs (binary + ternary) | `af3_inputs/binary/*.json`, `af3_inputs/ternary/*.json` |
| 7-8 | Batch JSONs + submit AF3 GPU jobs | `af3_output/binary/`, `af3_output/ternary/` |
| 9 | Analyze AF3 results (pLDDT, ipTM, RMSD) | `af3_analysis/master_metrics.csv` |
| 10 | Filter AF3 results by quality cutoffs | `af3_analysis/filtered_metrics.csv` |

**Monitor jobs during the run:**
```bash
# Check SLURM queue
squeue -u ryde3462

# Watch pipeline output (in another terminal)
# Pipeline prints progress to stdout as each stage completes
```

#### Running Stages Individually

If you want to run stages separately (e.g., after a failure or to re-run a step):

```bash
cd /projects/ryde3462/kyna_test

# Run MPNN + Rosetta + filter, stop before AF3 submission
python /projects/ryde3462/software/pyr1_pipeline/design/scripts/run_design_pipeline.py config.txt --skip-af3-submit --skip-af3-analyze --wait

# Skip MPNN/Rosetta, jump from existing Rosetta output to AF3 prep + submission
python /projects/ryde3462/software/pyr1_pipeline/design/scripts/run_design_pipeline.py config.txt --rosetta-to-af3 --wait

# Only prepare AF3 JSONs (no submission)
python /projects/ryde3462/software/pyr1_pipeline/design/scripts/run_design_pipeline.py config.txt --af3-prep-only

# Only batch and submit AF3 GPU jobs (JSONs already prepared)
python /projects/ryde3462/software/pyr1_pipeline/design/scripts/run_design_pipeline.py config.txt --af3-submit-only

# Only run AF3 analysis + filtering (after GPU jobs complete)
python /projects/ryde3462/software/pyr1_pipeline/design/scripts/run_design_pipeline.py config.txt --af3-analyze-only

# Dry run - generate all scripts but don't submit to SLURM
python /projects/ryde3462/software/pyr1_pipeline/design/scripts/run_design_pipeline.py config.txt --dry-run
```

#### All Design Pipeline Flags

| Flag | Effect |
|------|--------|
| `--wait` | Wait for each SLURM job batch to finish before continuing |
| `--dry-run` | Generate scripts but don't submit to SLURM |
| `--iteration N` | Run only iteration N |
| `--skip-mpnn` | Skip LigandMPNN stage |
| `--skip-rosetta` | Skip Rosetta relax stage |
| `--skip-aggregate` | Skip score aggregation |
| `--skip-filter` | Skip Rosetta filtering |
| `--skip-af3-prep` | Skip AF3 JSON generation |
| `--skip-af3-submit` | Stop before submitting AF3 GPU jobs |
| `--skip-af3-analyze` | Skip AF3 analysis and filtering |
| `--af3-prep-only` | Only do FASTA + AF3 JSON prep (skip MPNN/Rosetta) |
| `--af3-submit-only` | Only batch and submit AF3 GPU jobs |
| `--af3-analyze-only` | Only run AF3 analysis + quality filtering |
| `--rosetta-to-af3` | Skip MPNN/Rosetta, aggregate â†’ filter â†’ AF3 |

---

## ğŸ”§ Troubleshooting

### Issue: "Module not found" errors

**Solution:** Activate the correct conda environment

```bash
# For docking (needs PyRosetta)
module load anaconda
conda activate pyrosetta

# For MPNN
conda activate ligandmpnn_env
```

### Issue: "File not found" errors

**Check these paths in your config.txt:**

```bash
# Verify pipeline root exists
ls /projects/ryde3462/software/pyr1_pipeline

# Verify template PDB files exist
ls /projects/ryde3462/software/pyr1_pipeline/docking/ligand_alignment/files_for_PYR1_docking/

# Verify your SDF exists
ls /projects/ryde3462/kyna_test/conformers/kyna_conf.sdf
```

### Issue: SLURM jobs fail immediately

**Check logs:**
```bash
# Find your job ID
squeue -u ryde3462

# Check error log
cat docking_JOBID_0.err
cat docking_JOBID_0.out
```

### Issue: Out of disk space

**Check scratch usage:**
```bash
df -h /scratch/alpine/ryde3462
```

If full, clean up old outputs:
```bash
rm -rf /scratch/alpine/ryde3462/kyna_test/docked/array_*
# (Keep clustered_final though!)
```

---

## ğŸ“Š Expected Output Structure

After running the full pipeline on kyna_test, you should see:

```
kyna_test/                              (CAMPAIGN_ROOT = /projects/ryde3462/kyna_test)
â”œâ”€â”€ config.txt
â”œâ”€â”€ kyna_ligands.csv
â”œâ”€â”€ kyna_ligands.pkl
â”œâ”€â”€ conformers/
â”‚   â”œâ”€â”€ kyna_conf.sdf
â”‚   â””â”€â”€ 0/
â”‚       â”œâ”€â”€ 0.pdb
â”‚       â”œâ”€â”€ 0.params
â”‚       â””â”€â”€ 0.sdf

/scratch/alpine/ryde3462/kyna_test/     (SCRATCH_ROOT)
â”œâ”€â”€ docked/
â”‚   â”œâ”€â”€ array_0/                        (can delete after clustering)
â”‚   â”œâ”€â”€ array_1/
â”‚   â””â”€â”€ clustered_final/               â† Input to design pipeline
â”‚       â”œâ”€â”€ *.pdb
â”‚       â””â”€â”€ cluster_summary.csv
â”‚
â””â”€â”€ design/
    â”œâ”€â”€ iteration_1/
    â”‚   â”œâ”€â”€ mpnn_output/
    â”‚   â”‚   â”œâ”€â”€ submit_mpnn.sh          (auto-generated SLURM script)
    â”‚   â”‚   â””â”€â”€ array001_mpnn/*.fa
    â”‚   â”‚
    â”‚   â”œâ”€â”€ rosetta_output/
    â”‚   â”‚   â”œâ”€â”€ submit_rosetta.sh       (auto-generated SLURM script)
    â”‚   â”‚   â”œâ”€â”€ *.pdb                   (relaxed structures)
    â”‚   â”‚   â””â”€â”€ *.sc                    (score files)
    â”‚   â”‚
    â”‚   â”œâ”€â”€ scores/
    â”‚   â”‚   â””â”€â”€ iteration_1_scores.csv
    â”‚   â”‚
    â”‚   â””â”€â”€ filtered/
    â”‚       â”œâ”€â”€ filtered.csv            (top N designs)
    â”‚       â”œâ”€â”€ filtered.fasta          (sequences for AF3)
    â”‚       â””â”€â”€ *.pdb
    â”‚
    â”œâ”€â”€ af3_inputs/
    â”‚   â”œâ”€â”€ binary/
    â”‚   â”‚   â”œâ”€â”€ *.json                  (one per design)
    â”‚   â”‚   â”œâ”€â”€ batch_01/               (batched for GPU submission)
    â”‚   â”‚   â””â”€â”€ batch_02/
    â”‚   â””â”€â”€ ternary/
    â”‚       â”œâ”€â”€ *.json
    â”‚       â”œâ”€â”€ batch_01/
    â”‚       â””â”€â”€ batch_02/
    â”‚
    â”œâ”€â”€ af3_output/
    â”‚   â”œâ”€â”€ binary/
    â”‚   â”‚   â”œâ”€â”€ submit_af3_binary.sh    (auto-generated SLURM script)
    â”‚   â”‚   â”œâ”€â”€ logs/
    â”‚   â”‚   â””â”€â”€ */                      (AF3 prediction outputs)
    â”‚   â””â”€â”€ ternary/
    â”‚       â”œâ”€â”€ submit_af3_ternary.sh
    â”‚       â”œâ”€â”€ logs/
    â”‚       â””â”€â”€ */
    â”‚
    â””â”€â”€ af3_analysis/
        â”œâ”€â”€ binary_metrics.csv
        â”œâ”€â”€ ternary_metrics.csv
        â”œâ”€â”€ master_metrics.csv          â† Combined with ligand RMSD
        â””â”€â”€ filtered_metrics.csv        â† Final quality-filtered results
```

---

## âœ… Success Checklist

After each step, verify:

- [ ] **Step 1 (create_table):** `kyna_ligands.csv` exists and has rows
- [ ] **Step 2 (docking):** `/scratch/.../docked/clustered_final/` has PDB files
- [ ] **Step 3 (MPNN):** `/scratch/.../design/iteration_1/mpnn_output/` has `.fa` files
- [ ] **Step 4 (Rosetta):** `/scratch/.../design/iteration_1/rosetta_output/` has `.pdb` and `.sc` files
- [ ] **Step 5 (scores):** `/scratch/.../design/iteration_1/scores/iteration_1_scores.csv` exists
- [ ] **Step 6 (filtering):** `/scratch/.../design/iteration_1/filtered/` has filtered PDBs + `filtered.csv`
- [ ] **Step 7 (FASTA):** `/scratch/.../design/iteration_1/filtered/filtered.fasta` exists
- [ ] **Step 8 (AF3 prep):** `/scratch/.../design/af3_inputs/binary/` and `ternary/` have `.json` files
- [ ] **Step 9 (AF3 submit):** Jobs appear in `squeue -u ryde3462` (or already completed)
- [ ] **Step 10 (AF3 analysis):** `/scratch/.../design/af3_analysis/master_metrics.csv` exists
- [ ] **Step 11 (AF3 filter):** `/scratch/.../design/af3_analysis/filtered_metrics.csv` has passing designs

---

## ğŸ¯ Quick Command Reference

```bash
cd /projects/ryde3462/kyna_test
PIPE=/projects/ryde3462/software/pyr1_pipeline

# --- PHASE 1: Docking â†’ AF3 Prep (submit-and-forget, ~4 hours) ---

bash $PIPE/docking/scripts/submit_full_pipeline.sh config.txt \
  --design-args "--skip-af3-submit --skip-af3-analyze"

# Design only (docking already done)
bash $PIPE/docking/scripts/submit_full_pipeline.sh config.txt \
  --design-only --design-args "--skip-af3-submit --skip-af3-analyze"

# Docking only
bash $PIPE/docking/scripts/submit_full_pipeline.sh config.txt --docking-only

# --- PHASE 2: Submit AF3 GPU Jobs (from login node, takes seconds) ---

python $PIPE/design/scripts/run_design_pipeline.py config.txt --af3-submit-only

# --- PHASE 3: Analyze AF3 Results (after GPU jobs finish) ---

python $PIPE/design/scripts/run_design_pipeline.py config.txt --af3-analyze-only

# --- MONITORING ---

tail -f /scratch/alpine/ryde3462/kyna_test/logs/pipeline_orchestrator_*.out
cat /scratch/alpine/ryde3462/kyna_test/logs/pipeline_status.log
squeue -u ryde3462

# --- DOCKING ONLY ---

# Submit docking via SLURM (returns immediately)
python $PIPE/docking/scripts/run_docking_workflow.py config.txt --slurm

# Submit docking via SLURM and wait for completion
python $PIPE/docking/scripts/run_docking_workflow.py config.txt --slurm --wait

# Test locally (no SLURM)
python $PIPE/docking/scripts/run_docking_workflow.py config.txt --local-arrays 2

# --- DESIGN PIPELINE ONLY (after docking completes) ---

# Run everything: MPNN â†’ Rosetta â†’ Filter â†’ AF3 prep â†’ AF3 submit â†’ AF3 analyze
python $PIPE/design/scripts/run_design_pipeline.py config.txt --wait

# Dry run (see what would happen without submitting jobs)
python $PIPE/design/scripts/run_design_pipeline.py config.txt --dry-run

# --- PARTIAL DESIGN RUNS ---

# MPNN + Rosetta + filter only (stop before AF3)
python $PIPE/design/scripts/run_design_pipeline.py config.txt --skip-af3-submit --skip-af3-analyze --wait

# Jump from existing Rosetta output â†’ AF3
python $PIPE/design/scripts/run_design_pipeline.py config.txt --rosetta-to-af3 --wait

# Submit AF3 jobs only (JSONs already prepared)
python $PIPE/design/scripts/run_design_pipeline.py config.txt --af3-submit-only

# Analyze AF3 results only (after GPU jobs complete)
python $PIPE/design/scripts/run_design_pipeline.py config.txt --af3-analyze-only

# --- MONITORING ---
squeue -u ryde3462
```

---

## ğŸ“– Next Steps

Once kyna_test works:

1. **Create a new project directory** for your real ligand
2. **Copy the config template** and customize paths
3. **Run the full pipeline** with confidence!

Example for a new project:
```bash
mkdir /projects/ryde3462/my_new_ligand
cd /projects/ryde3462/my_new_ligand
mkdir conformers
cp /projects/ryde3462/software/pyr1_pipeline/templates/unified_config_template.txt config.txt
nano config.txt  # Edit CAMPAIGN_ROOT, SCRATCH_ROOT, etc.
```

Happy docking! ğŸš€
